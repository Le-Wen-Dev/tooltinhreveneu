# Docker Setup Guide

## ğŸ“ Cáº¥u TrÃºc Project

```
revenue-crawler/
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ main.py          # Crawler service
â”‚   â”œâ”€â”€ db.py            # Database models
â”‚   â”œâ”€â”€ lock.py          # Lock mechanism
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py          # FastAPI service
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ database_schema_complete.sql
```

## ğŸš€ Setup

### 1. Táº¡o file .env

```bash
cp .env.example .env
# Sá»­a thÃ´ng tin database trong .env
```

### 2. Build vÃ  cháº¡y

```bash
# Build images
docker compose build

# Cháº¡y API service (background)
docker compose up -d api db

# Test crawler (manual)
docker compose run --rm crawler

# Hoáº·c vá»›i date cá»¥ thá»ƒ
docker compose run --rm crawler --date 2026-01-26
```

## â± Cron Job Setup (VPS)

### Táº¡o cron job

```bash
crontab -e
```

ThÃªm dÃ²ng:

```bash
# Cháº¡y crawler má»—i ngÃ y lÃºc 1:00 AM vÃ  1:00 PM
0 1,13 * * * cd /srv/revenue-crawler && docker compose run --rm crawler >> /var/log/revenue-crawler.log 2>&1
```

### Hoáº·c script wrapper

Táº¡o file `/srv/revenue-crawler/run-crawler.sh`:

```bash
#!/bin/bash
cd /srv/revenue-crawler
docker compose run --rm crawler >> /var/log/revenue-crawler.log 2>&1
```

Chmod:
```bash
chmod +x /srv/revenue-crawler/run-crawler.sh
```

Cron:
```bash
0 1,13 * * * /srv/revenue-crawler/run-crawler.sh
```

## ğŸ”’ Báº£o Máº­t

### 1. MySQL khÃ´ng má»Ÿ public

Trong `docker-compose.yml`, bá» port mapping náº¿u khÃ´ng cáº§n:
```yaml
# Bá» dÃ²ng nÃ y náº¿u khÃ´ng cáº§n access tá»« ngoÃ i
ports:
  - "3306:3306"
```

### 2. DB user riÃªng cho crawler

ÄÃ£ cÃ³ trong schema:
- User: `tooltinhreveneu_1`
- Chá»‰ cÃ³ quyá»n trÃªn database `tooltinhreveneu_1`

### 3. Log ra file

Logs Ä‘Æ°á»£c lÆ°u vÃ o:
- `/app/logs/crawler.log` (trong container)
- Mount vÃ o `./logs` (host)

### 4. Lock mechanism

Crawler tá»± Ä‘á»™ng lock Ä‘á»ƒ trÃ¡nh cháº¡y trÃ¹ng:
- Table: `crawl_runs`
- Check PID Ä‘á»ƒ detect stale locks

### 5. Retry logic

CÃ³ thá»ƒ thÃªm retry trong `crawler/main.py` náº¿u cáº§n.

## ğŸ“Š Workflow

```
1. Cron trigger â†’ docker compose run crawler
2. Crawler fetch data â†’ lÆ°u raw_revenue_data
3. Calculator tÃ­nh formulas â†’ lÆ°u computed_metrics & aggregated_metrics
4. API query metrics â†’ tráº£ JSON
```

## ğŸ§ª Test

### Test crawler:
```bash
docker compose run --rm crawler --first-page-only
```

### Test API:
```bash
# Start API
docker compose up -d api

# Test endpoints
curl http://localhost:8000/health
curl http://localhost:8000/api/aggregated-metrics
curl http://localhost:8000/api/fetch-logs
```

### Test database:
```bash
docker compose exec db mysql -u tooltinhreveneu_1 -p tooltinhreveneu_1
```

## ğŸ”„ Update/Deploy

```bash
# Pull code má»›i
git pull

# Rebuild images
docker compose build

# Restart services
docker compose up -d --force-recreate api

# Crawler tá»± Ä‘á»™ng dÃ¹ng image má»›i khi cron cháº¡y
```

## ğŸ“ Production Checklist

- [ ] `.env` file vá»›i credentials Ä‘Ãºng
- [ ] Database schema Ä‘Ã£ import
- [ ] Cron job Ä‘Ã£ setup
- [ ] Logs directory cÃ³ quyá»n write
- [ ] MySQL khÃ´ng expose ra ngoÃ i
- [ ] API cÃ³ reverse proxy (nginx) náº¿u cáº§n
- [ ] Backup strategy cho database volume

## ğŸ› Troubleshooting

### Crawler khÃ´ng cháº¡y:
```bash
# Check logs
docker compose logs crawler
tail -f /var/log/revenue-crawler.log

# Test manual
docker compose run --rm crawler --first-page-only
```

### API khÃ´ng káº¿t ná»‘i DB:
```bash
# Check DB connection
docker compose exec api python -c "from crawler.db import engine; engine.connect()"

# Check logs
docker compose logs api
```

### Lock khÃ´ng release:
```sql
-- Manual cleanup
DELETE FROM crawl_runs WHERE status = 'running' AND completed_at IS NULL;
```
