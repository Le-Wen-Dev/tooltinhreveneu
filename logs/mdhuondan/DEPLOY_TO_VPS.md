# HÆ°á»›ng Dáº«n Deploy LÃªn VPS

## âœ… Images ÄÃ£ Build Xong

- `toolgetdata-crawler:latest`
- `toolgetdata-api:latest`

## ğŸš€ BÆ°á»›c 1: Save Images (ÄÃ£ lÃ m xong)

```bash
./save-and-push-images.sh
```

Images sáº½ Ä‘Æ°á»£c save thÃ nh:
- `crawler-image.tar`
- `api-image.tar`

## ğŸ“¤ BÆ°á»›c 2: Upload Images LÃªn VPS

```bash
# Upload images
scp -P 2222 crawler-image.tar api-image.tar tooltinhreveneu@gmail.com@36.50.27.158:/srv/toolgetdata/

# Upload code vÃ  config
scp -P 2222 -r docker-compose.yml database_schema_complete.sql tooltinhreveneu@gmail.com@36.50.27.158:/srv/toolgetdata/
```

## ğŸ”§ BÆ°á»›c 3: Setup TrÃªn VPS

### SSH vÃ o VPS:
```bash
ssh tooltinhreveneu@gmail.com@36.50.27.158 -p 2222
cd /srv/toolgetdata
```

### Load Images:
```bash
docker load -i crawler-image.tar
docker load -i api-image.tar

# Verify
docker images | grep toolgetdata
```

### Import Database Schema:
```bash
mysql -u tooltinhreveneu_1 -p tooltinhreveneu_1 < database_schema_complete.sql
# Password: tooltinhreveneu@gndhsggkl
```

### Táº¡o .env:
```bash
cat > .env << EOF
DB_TYPE=mysql
DB_HOST=localhost
DB_PORT=3306
DB_NAME=tooltinhreveneu_1
DB_USER=tooltinhreveneu_1
DB_PASSWORD=tooltinhreveneu@gndhsggkl
SCRAPER_USERNAME=maxvaluemedia
SCRAPER_PASSWORD=gliacloud
API_PORT=8000
EOF
```

### Táº¡o logs directory:
```bash
mkdir -p logs
chmod 755 logs
```

## ğŸ¯ BÆ°á»›c 4: Cháº¡y Services

### Start API:
```bash
docker compose up -d api
```

### Test Crawler:
```bash
docker compose run --rm crawler --first-page-only
```

### Verify API:
```bash
curl http://localhost:8000/health
curl http://localhost:8000/api/aggregated-metrics
```

## â± BÆ°á»›c 5: Setup Cron

```bash
crontab -e
```

ThÃªm:
```bash
# Cháº¡y crawler 2 láº§n má»—i ngÃ y: 1:00 AM vÃ  1:00 PM
0 1,13 * * * cd /srv/toolgetdata && docker compose run --rm crawler >> /var/log/revenue-crawler.log 2>&1
```

## âœ… Checklist

- [ ] Images Ä‘Ã£ upload lÃªn VPS
- [ ] Images Ä‘Ã£ load vÃ o Docker
- [ ] Database schema Ä‘Ã£ import
- [ ] File .env Ä‘Ã£ táº¡o
- [ ] API service Ä‘Ã£ cháº¡y
- [ ] Crawler test thÃ nh cÃ´ng
- [ ] Cron job Ä‘Ã£ setup

## ğŸ”„ Update Sau NÃ y

Khi cÃ³ code má»›i:

1. **Build láº¡i images** (trÃªn mÃ¡y local):
   ```bash
   docker compose build
   ./save-and-push-images.sh
   ```

2. **Upload lÃªn VPS**:
   ```bash
   scp -P 2222 crawler-image.tar api-image.tar tooltinhreveneu@gmail.com@36.50.27.158:/srv/toolgetdata/
   ```

3. **Load vÃ  restart** (trÃªn VPS):
   ```bash
   docker load -i crawler-image.tar
   docker load -i api-image.tar
   docker compose up -d --force-recreate api
   ```

## ğŸ“ Vá» MySQL

**KHÃ”NG Cáº¦N** setup MySQL container vÃ¬:
- âœ… MySQL Ä‘Ã£ cÃ³ sáºµn trÃªn VPS
- âœ… Database `tooltinhreveneu_1` Ä‘Ã£ tá»“n táº¡i
- âœ… Chá»‰ cáº§n import schema 1 láº§n

Xem chi tiáº¿t: `MYSQL_INFO.md`
