# ğŸ“‹ TÃ³m Táº¯t Dá»± Ãn - Revenue Data Crawler & API

## âœ… ÄÃ£ HoÃ n ThÃ nh

### 1. **Web Scraper**
- âœ… Login vÃ o `https://gstudio.gliacloud.com` vá»›i CSRF token handling
- âœ… Scrape data tá»« HTML table (pagination support)
- âœ… Parse cÃ¡c fields: channel, slot, time_unit, total_player_impr, total_ad_impr, rpm, net_revenue_usd
- âœ… Test mode: `--first-page-only` Ä‘á»ƒ test nhanh

### 2. **Database Schema (MySQL)**
- âœ… 7 tables:
  - `raw_revenue_data`: Dá»¯ liá»‡u thÃ´ tá»« scraper
  - `formulas`: Äá»‹nh nghÄ©a cÃ´ng thá»©c tÃ­nh toÃ¡n
  - `computed_metrics`: Káº¿t quáº£ tÃ­nh toÃ¡n theo tá»«ng row
  - `aggregated_metrics`: Káº¿t quáº£ tá»•ng há»£p theo channel/time_unit
  - `fetch_logs`: Lá»‹ch sá»­ fetch data
  - `crawl_runs`: Lock mechanism Ä‘á»ƒ trÃ¡nh cháº¡y trÃ¹ng
  - `admin_users`: Quáº£n lÃ½ admin (cho admin panel)
- âœ… Schema tá»± Ä‘á»™ng import khi MySQL container khá»Ÿi Ä‘á»™ng

### 3. **Formula Engine**
- âœ… TÃ­nh toÃ¡n cÃ¡c metrics:
  - `rpm_total_net_revenue`: Tá»•ng Net Revenue (Mobile + Desktop)
  - `rpm_per_1000_players`: (Net Revenue / Total Player Impressions) * 1000
  - `rpm_combined`: (Tá»•ng Net Revenue / Tá»•ng Player Impressions) * 1000
  - `total_net_revenue`: Tá»•ng Net Revenue
- âœ… Focus 100% vÃ o Net Revenue (ignore Gross Revenue)

### 4. **API (FastAPI)**
- âœ… Endpoints:
  - `GET /health`: Health check
  - `GET /api/raw-data`: Láº¥y raw data (filter: channel, time_unit, fetch_date)
  - `GET /api/computed-metrics`: Láº¥y computed metrics
  - `GET /api/aggregated-metrics`: Láº¥y aggregated metrics
  - `GET /api/fetch-logs`: Lá»‹ch sá»­ fetch
  - `GET /api/formulas`: Danh sÃ¡ch formulas
  - `POST /api/formulas`: Táº¡o formula má»›i
  - `PUT /api/formulas/{id}`: Cáº­p nháº­t formula
  - `DELETE /api/formulas/{id}`: XÃ³a formula
  - `POST /api/compute-metrics`: Trigger tÃ­nh toÃ¡n láº¡i metrics

### 5. **Admin Panel**
- âœ… Web interface Ä‘á»ƒ quáº£n lÃ½ formulas
- âœ… CRUD operations cho formulas
- âœ… Access táº¡i: `http://localhost:8000/admin/formulas`

### 6. **Docker Setup**
- âœ… 3 services:
  - `crawler`: Service crawl data (cháº¡y xong exit)
  - `api`: FastAPI service (cháº¡y liÃªn tá»¥c)
  - `db`: MySQL 8.0 container (tá»± Ä‘á»™ng import schema)
- âœ… Docker Compose vá»›i health checks
- âœ… Network isolation
- âœ… Volume persistence cho database

### 7. **Data Fetching Logic**
- âœ… Update/Insert: Tá»± Ä‘á»™ng update náº¿u record Ä‘Ã£ tá»“n táº¡i (dá»±a trÃªn channel, slot, time_unit, fetch_date)
- âœ… Tá»± Ä‘á»™ng tÃ­nh toÃ¡n metrics sau khi fetch xong
- âœ… Logging vÃ o `fetch_logs` table

### 8. **Concurrency Control**
- âœ… `crawl_runs` table Ä‘á»ƒ lock
- âœ… TrÃ¡nh cháº¡y trÃ¹ng crawler cho cÃ¹ng má»™t date

## ğŸ—ï¸ Cáº¥u TrÃºc Project

```
toolgetdata/
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ main.py          # Entry point cho crawler
â”‚   â”œâ”€â”€ db.py            # Database models
â”‚   â”œâ”€â”€ lock.py          # Lock mechanism
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py          # FastAPI app
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py           # Shared models & logic
â”‚   â”œâ”€â”€ data_fetcher.py  # Orchestration logic
â”‚   â”œâ”€â”€ formula_engine.py # Formula calculation
â”‚   â”œâ”€â”€ scraper.py       # Web scraping
â”‚   â”œâ”€â”€ admin_panel.py   # Admin panel routes
â”‚   â””â”€â”€ templates/       # HTML templates
â”œâ”€â”€ database_schema_complete.sql  # MySQL schema
â”œâ”€â”€ docker-compose.yml   # Docker Compose config
â”œâ”€â”€ .env                 # Environment variables (local)
â”œâ”€â”€ .env.production      # Environment variables (production)
â”œâ”€â”€ start-local-test.sh  # Script test local
â””â”€â”€ DEPLOY_VPS_DOCKER_DB.md  # HÆ°á»›ng dáº«n deploy
```

## ğŸš€ Local Testing

```bash
# Start all services
docker compose up -d

# Test crawler (first page only)
docker compose run --rm crawler --first-page-only

# Test API
curl http://localhost:8000/health
curl http://localhost:8000/api/raw-data
```

## ğŸ“¦ Deploy LÃªn VPS

### BÆ°á»›c 1: Build & Save Images
```bash
./save-and-push-images.sh
```

### BÆ°á»›c 2: Upload LÃªn VPS
```bash
scp -P 2222 \
  crawler-image.tar \
  api-image.tar \
  docker-compose.yml \
  database_schema_complete.sql \
  .env.production \
  user@vps:/srv/toolgetdata/
```

### BÆ°á»›c 3: SSH & Deploy
```bash
ssh user@vps -p 2222
cd /srv/toolgetdata

# Load images
docker load -i crawler-image.tar
docker load -i api-image.tar

# Setup .env
cp .env.production .env

# Start services
docker compose up -d

# Verify
curl http://localhost:8000/health
```

### BÆ°á»›c 4: Setup Cron
```bash
crontab -e

# ThÃªm:
0 1,13 * * * cd /srv/toolgetdata && docker compose run --rm crawler >> /var/log/revenue-crawler.log 2>&1
```

## ğŸ”§ Environment Variables

```bash
# Database (dÃ¹ng MySQL container)
DB_TYPE=mysql
DB_HOST=db              # Service name trong Docker
DB_PORT=3306
DB_NAME=tooltinhreveneu_1
DB_USER=tooltinhreveneu_1
DB_PASSWORD=your_password
DB_ROOT_PASSWORD=rootpassword

# Scraper
SCRAPER_USERNAME=maxvaluemedia
SCRAPER_PASSWORD=gliacloud

# API
API_PORT=8000
```

## ğŸ“Š Database Tables

1. **raw_revenue_data**: Dá»¯ liá»‡u thÃ´ tá»« scraper
2. **formulas**: CÃ´ng thá»©c tÃ­nh toÃ¡n
3. **computed_metrics**: Metrics tÃ­nh theo row
4. **aggregated_metrics**: Metrics tá»•ng há»£p
5. **fetch_logs**: Lá»‹ch sá»­ fetch
6. **crawl_runs**: Lock mechanism
7. **admin_users**: Admin users

## ğŸ¯ API Endpoints

### Health
- `GET /health`

### Raw Data
- `GET /api/raw-data?channel=...&time_unit=...&fetch_date=...`

### Metrics
- `GET /api/computed-metrics?metric_name=...`
- `GET /api/aggregated-metrics?formula_name=...&channel=...`

### Formulas
- `GET /api/formulas`
- `POST /api/formulas`
- `PUT /api/formulas/{id}`
- `DELETE /api/formulas/{id}`

### Admin
- `GET /admin/formulas` - List formulas
- `GET /admin/formulas/new` - Create formula
- `GET /admin/formulas/{id}/edit` - Edit formula

## âœ… Advantages cá»§a MySQL Container

- âœ… KhÃ´ng cáº§n setup MySQL trÃªn VPS
- âœ… Schema tá»± Ä‘á»™ng import khi khá»Ÿi Ä‘á»™ng
- âœ… Database persist trong Docker volume
- âœ… Dá»… backup/restore
- âœ… Isolated, khÃ´ng áº£nh hÆ°á»Ÿng MySQL khÃ¡c
- âœ… Dá»… migrate/update

## ğŸ”„ Workflow

1. **Cron trigger** â†’ `docker compose run --rm crawler`
2. **Crawler** â†’ Login â†’ Scrape â†’ Save to DB â†’ Compute metrics
3. **API** â†’ Expose data qua REST endpoints
4. **Admin Panel** â†’ Quáº£n lÃ½ formulas

## ğŸ“ Notes

- Crawler cháº¡y xong sáº½ exit (khÃ´ng restart)
- API cháº¡y liÃªn tá»¥c (restart unless-stopped)
- MySQL container tá»± Ä‘á»™ng import schema tá»« `database_schema_complete.sql`
- Database volume: `toolgetdata_db_data` (persist data)

## ğŸ› Troubleshooting

### MySQL khÃ´ng import schema:
```bash
docker compose logs db
docker compose exec db ls -la /docker-entrypoint-initdb.d/
```

### Containers khÃ´ng káº¿t ná»‘i DB:
```bash
docker network inspect toolgetdata_revenue-network
docker compose exec api python -c "from crawler.db import engine; engine.connect()"
```

### Reset database:
```bash
docker compose down -v  # XÃ³a volume
docker compose up -d     # Táº¡o láº¡i vÃ  import schema
```
